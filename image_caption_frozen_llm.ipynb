{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import LlamaModel, LlamaTokenizer\n",
    "\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self, qformer, llm_tokenizer, llm_model):\n",
    "        super(VisionLanguageModel, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # ViT model\n",
    "        self.qformer = qformer\n",
    "        for param in self.qformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Llama model\n",
    "        self.llm_model = llm_model\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Linear layer\n",
    "        embedding_size = self.llm_model.config.hidden_size\n",
    "        # self.linear = nn.Linear(256, embedding_size)\n",
    "        self.linear = ProcessingLayer(256, embedding_size)\n",
    "\n",
    "        # Tokenizer\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "\n",
    "        # Move entire model to device, instead of moving parts individually\n",
    "        self.to(self.device)\n",
    "\n",
    "        # Embeddings for <image> and </image>\n",
    "        self.image_start_token_id = self.llm_tokenizer.encode('<image>', add_special_tokens=False)[0]\n",
    "        self.image_end_token_id = self.llm_tokenizer.encode('</image>', add_special_tokens=False)[0]\n",
    "\n",
    "        # Create a prompt for GPT model\n",
    "        prompt = \"Describe this image: \"\n",
    "        # prompt = \"this is a picture of \"\n",
    "        self.prompt_tokens = self.llm_tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "\n",
    "    def forward(self, image, caption):\n",
    "        batch_size = image.shape[0]\n",
    "\n",
    "        # Embeddings for <image> and </image> and prompt\n",
    "        image_start_embedding = self.llm_model.base_model.embed_tokens(torch.tensor([self.image_start_token_id], device=self.device)).repeat(batch_size, 1, 1)\n",
    "        image_end_embedding = self.llm_model.base_model.embed_tokens(torch.tensor([self.image_end_token_id], device=self.device)).repeat(batch_size, 1, 1)\n",
    "        prompt_embeddings = self.llm_model.base_model.embed_tokens(self.prompt_tokens).repeat(batch_size, 1, 1)\n",
    "\n",
    "        # Ensure image is on the correct device\n",
    "        image = image.to(self.device)\n",
    "\n",
    "        # Get image features\n",
    "        features_image = self.qformer.extract_features({'image':image}, mode=\"image\")\n",
    "\n",
    "        # Pass through linear layer\n",
    "        linear_output = self.linear(features_image.image_embeds_proj)\n",
    "        # print('linear_output.shape: ', linear_output.shape)\n",
    "\n",
    "        # Concatenate the prompt, image start embedding, image features, and image end embedding\n",
    "        cond_embedding = torch.cat([prompt_embeddings, image_start_embedding, linear_output, image_end_embedding], dim=1)\n",
    "\n",
    "        # Prepare target token ids for LLM model\n",
    "        self.llm_tokenizer.padding_side = \"right\"\n",
    "        self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token\n",
    "        regression_target = self.llm_tokenizer(caption, return_tensors=\"pt\", padding='longest', truncation=True, max_length=32, add_special_tokens=False)\n",
    "        regression_target_ids = regression_target.input_ids\n",
    "        regression_attention_mask = regression_target.attention_mask\n",
    "        part_targets = regression_target_ids.masked_fill(regression_target_ids == self.llm_tokenizer.eos_token_id, -100)\n",
    "\n",
    "        regression_embs = self.llm_model.base_model.embed_tokens(regression_target_ids.to(self.device))\n",
    "\n",
    "        # concat the embedding to condition and the embedding to regress\n",
    "        cat_embs = torch.cat([cond_embedding, regression_embs], dim=1)\n",
    "        cat_att = torch.cat([torch.ones((batch_size, cond_embedding.shape[1]), dtype=torch.long), regression_attention_mask], dim=1).to(self.device)\n",
    "\n",
    "        # get bos token embedding\n",
    "        bos_token_id = self.llm_tokenizer.bos_token_id\n",
    "        bos_token_emb = self.llm_model.base_model.embed_tokens(torch.tensor([bos_token_id], device=self.device)).repeat(batch_size, 1, 1)\n",
    "        bos_att = torch.ones((batch_size, 1), dtype=torch.long, device=self.device)\n",
    "\n",
    "        # concat bos token embedding to condition and attention mask\n",
    "        cat_embs = torch.cat([bos_token_emb, cat_embs], dim=1)\n",
    "        cat_att = torch.cat([bos_att, cat_att], dim=1)\n",
    "        # print('cat_embs.shape: ', cat_embs.shape)\n",
    "\n",
    "        # final target token ids\n",
    "        targets = torch.ones((batch_size, cat_embs.shape[1]), dtype=torch.long).fill_(-100)\n",
    "        targets[:, 42:] = part_targets\n",
    "        targets[:, 0] = bos_token_id\n",
    "\n",
    "        self.llm_model.eval()\n",
    "        outputs = self.llm_model(inputs_embeds=cat_embs, attention_mask=cat_att, labels=targets)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, image):\n",
    "        batch_size = image.shape[0]\n",
    "        prompt_embeddings = self.llm_model.base_model.embed_tokens(self.prompt_tokens).repeat(batch_size, 1, 1)\n",
    "        image_start_embedding = self.llm_model.base_model.embed_tokens(torch.tensor([self.image_start_token_id], device=self.device)).repeat(batch_size, 1, 1)\n",
    "        image_end_embedding = self.llm_model.base_model.embed_tokens(torch.tensor([self.image_end_token_id], device=self.device)).repeat(batch_size, 1, 1)\n",
    "\n",
    "        image = image.to(self.device)\n",
    "        features_image = self.qformer.extract_features({'image':image}, mode=\"image\")\n",
    "        with torch.no_grad():\n",
    "            linear_output = self.linear(features_image.image_embeds_proj)\n",
    "        image_embedding = torch.cat([prompt_embeddings, image_start_embedding, linear_output, image_end_embedding], dim=1)\n",
    "        attention_mask = torch.ones((batch_size, image_embedding.shape[1]), dtype=torch.long).to(self.device)\n",
    "        self.llm_model.eval()\n",
    "        outputs = self.llm_model.generate(inputs_embeds=image_embedding, attention_mask = attention_mask, max_length=50, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1)\n",
    "        outputs = self.llm_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        return outputs\n",
    "\n",
    "class ProcessingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProcessingLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 512)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(512, 1024)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(1024, output_dim)\n",
    "        self.layernorm = nn.LayerNorm(output_dim)\n",
    "        self.activation3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.activation3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pytorch pre-release version 2.1.0a0+29c30b1 - assuming intent to test it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec8c3167916408b9c2bde723475cfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from lavis.models import load_model_and_preprocess\n",
    "import torch\n",
    "\n",
    "checkpoint_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "llm_model = LlamaForCausalLM.from_pretrained(checkpoint_path)\n",
    "llm_tokenizer = LlamaTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "qformer, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_feature_extractor\", model_type=\"pretrain\", is_eval=True, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 2 tokens\n",
      "[1, 29871, 32000]\n",
      "[1, 29871, 32001]\n"
     ]
    }
   ],
   "source": [
    "# Add new tokens\n",
    "new_tokens = ['<image>', '</image>']\n",
    "num_added_toks = llm_tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "print(\"We have added\", num_added_toks, \"tokens\")\n",
    "\n",
    "# Resize position embeddings matrix\n",
    "llm_model.resize_token_embeddings(len(llm_tokenizer))\n",
    "\n",
    "# Check if new tokens are recognized\n",
    "print(llm_tokenizer.encode('<image>'))\n",
    "print(llm_tokenizer.encode('</image>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = VisionLanguageModel(qformer, llm_tokenizer, llm_model)\n",
    "model.linear.load_state_dict(torch.load('linear_layer_state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTImageProcessor, GPT2Tokenizer\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, json_file, image_dir, transform=None):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)[\"annotations\"]\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.vis_processors = vis_processors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image\n",
    "        image_path = f\"{self.image_dir}/{self.data[idx]['image_id']}.jpg\"\n",
    "        images = Image.open(image_path).convert(\"RGB\").resize((224, 224))  # Read image and resize to 224x224\n",
    "        image_tensor = self.vis_processors[\"eval\"](images)\n",
    "        # Get caption\n",
    "        caption = self.data[idx]['caption']\n",
    "        return {'image': image_tensor, 'caption': caption}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dataset\n",
    "dataset = CustomDataset(json_file='cc_sbu_align/filter_cap.json', image_dir='cc_sbu_align/image')\n",
    "\n",
    "# DataLoader for training, with batching and shuffling\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from accelerate import Accelerator\n",
    "\n",
    "learning_rate = 1e-3\n",
    "# optimizer with weight decay\n",
    "optimizer = AdamW(model.linear.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "loss_function = CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Train mode\n",
    "# model.train()\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "model, optimizer, dataloader = accelerator.prepare(\n",
    "    model, optimizer, dataloader\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: This image shows a dachshund wiper wiping the dust on a car during a race. The wiper is orange and black and has a curved blade that appears to be made of metal. The car is\n",
      "Generated output: The image shows a statue of a Greek goddess, possibly Athena, made of gold and standing on a hill overlooking a city at night. The city is illuminated by bright lights and there are people walking around in the foreground\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "raw_image1 = Image.open(\"dog.jpg\").convert(\"RGB\")\n",
    "raw_image2 = Image.open(\"merlion.png\").convert(\"RGB\")\n",
    "image1 = vis_processors[\"eval\"](raw_image1).unsqueeze(0).to(device)\n",
    "image2 = vis_processors[\"eval\"](raw_image2).unsqueeze(0).to(device)\n",
    "input_test = torch.cat([image1, image2], dim=0)\n",
    "answer = model.generate(input_test)\n",
    "for i in range(len(answer)):\n",
    "    print('Generated output:', answer[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/108], Loss: 0.1691\n",
      "Output_generate:  This image is a yellow and black Dachsh lawnmower with a yellow and black body and orange wheels. The lawnmower has a bright orange blade that matches the body color. The mower is running on a green lawn, indicating that it is being used to maintain the grass. A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A: A:\n",
      "Output_generate:  The image shows a statue of a lion in the middle of a fountain in front of a large building in a city. The fountain appears to be made of gold or silver and the statue of the lion is also made of gold or silver. The building in the background appears to be made of glass or reflective material and has a large dome on top. The image appears to be taken at night, as the lights from the city can be seen in the background. The lion statue appears to be watching over the city and the fountain below. What is the main focus of the\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/vision-language-model/image_caption_frozen_llm.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Beuclid/workspaces/vision-language-model/image_caption_frozen_llm.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Backpropagate, optimizer step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Beuclid/workspaces/vision-language-model/image_caption_frozen_llm.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# loss.backward()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Beuclid/workspaces/vision-language-model/image_caption_frozen_llm.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m accelerator\u001b[39m.\u001b[39mbackward(loss)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Beuclid/workspaces/vision-language-model/image_caption_frozen_llm.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Beuclid/workspaces/vision-language-model/image_caption_frozen_llm.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656665726e616e6465732f646174612f656665726e616e6465732f70726f6a656374732f766973696f6e2d6c616e67756167652d6d6f64656c2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Beuclid/workspaces/vision-language-model/image_caption_frozen_llm.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# Print loss every 10 batches for monitoring\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/optimizer.py:132\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, closure)\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    136\u001b[0m         \u001b[39m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:374\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    372\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 374\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    376\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[1;32m    378\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:289\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    288\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39;49m(v\u001b[39m.\u001b[39;49mitem() \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m optimizer_state[\u001b[39m\"\u001b[39;49m\u001b[39mfound_inf_per_device\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues()):\n\u001b[1;32m    290\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:289\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    288\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39;49mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    290\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0 # For monitoring training loss\n",
    "    for i, batch in enumerate(dataloader):  # Assuming dataloader is already defined\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch['image'].to(device)\n",
    "        # print(images)\n",
    "        captions = batch['caption']\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(images, captions)\n",
    "        loss = output.loss\n",
    "        \n",
    "        # Backpropagate, optimizer step\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 10 == 0:  # Print loss every 10 batches for monitoring\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "            # print a sample output\n",
    "            output_generate = model.generate(torch.cat([image1, image2], dim=0))\n",
    "            for i in range(len(output_generate)):\n",
    "                output_generate[i] = output_generate[i].replace('\\n', ' ')\n",
    "                print('Output_generate: ', output_generate[i])\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss}\")\n",
    "    # Save the state dictionary of the linear layer\n",
    "    torch.save(model.linear.state_dict(), 'linear_layer_state_dict.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
